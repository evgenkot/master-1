"""
Имеется сигмоидная нейронная сеть. Задана обучающая выборка из одного прецедента: V={([2;3],1)}. Заданы начальные веса и смещение:
w1=0.5;
w2=3;
b=-9.

Используя функцию перекрестной энтропии в качестве функции потерь и регуляризацию L2 выполнить одну эпоху обучения. Скорость обучения положить равной 0.4; регуляризационный параметр положить равным 0.1. В качестве ответа ввести новое значение a. Допускается округление ответа до 3 знаков после запятой.

Указание.
Для нахождения
∂C/∂w1,
∂C/∂w2
и ∂C∂b

использовать следующие формулы метода обратного распространения ошибки:
BP1"    δ^Li=ai−yi;
BP3     ∂C/∂b^li=δli;
BP4     ∂C/∂w^lij=δ^li⋅a^(l−1)j.

Для минимизации стоимостной функции использовать
градиентный спуск с регуляризацией L2:
wq:=(1−ηλ/|V|)wq−η*∂C/∂wq;
b:=b−η*∂C/∂b.

Справочно:
сигмоидная функция: σ(z)=1/(1+e^−z).
"""

import numpy as np

# Заданные параметры
w1 = 0.5
w2 = 3.0
b = -9.0
learning_rate = 0.4
lambda_reg = 0.1
X = np.array([2, 3])
y = 1


# Сигмоидная функция
def sigmoid(z):
    return 1 / (1 + np.exp(-z))


# Прямое распространение
z = w1 * X[0] + w2 * X[1] + b
a = sigmoid(z)

# Вычисление градиентов
delta_L = a - y  # BP1
dC_db = delta_L  # BP3
dC_dw1 = delta_L * X[0]  # BP4
dC_dw2 = delta_L * X[1]  # BP4

# Обновление весов и смещения с регуляризацией L2
w1 = (1 - learning_rate * lambda_reg) * w1 - learning_rate * dC_dw1
w2 = (1 - learning_rate * lambda_reg) * w2 - learning_rate * dC_dw2
b = b - learning_rate * dC_db

# Новое значение a после обновления весов
z_new = w1 * X[0] + w2 * X[1] + b
a_new = sigmoid(z_new)

# Округление результата до 3 знаков после запятой

print("Новое значение a:", a_new)
